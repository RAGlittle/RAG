services:
  text-embeddings-medium:
    container_name : embedding-inference-medium
    # depending on GPU architecture you may need to change the prefix for the version tag, on older
    # GPUs you may need to use `turing-<VERSION>
    # FIXME: downgrading the image to 1.4 seems to fix the issue with https://github.com/huggingface/text-embeddings-inference/issues/347
    image: ghcr.io/huggingface/text-embeddings-inference:turing-1.4-grpc
    command: --model-id  Alibaba-NLP/gte-large-en-v1.5 --json-output
    volumes:
      - embedding_data:/data 
    ports:
      - "8080:80" 
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    pull_policy: always
    # TODO : grpc healthcheck
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
    #   interval: 30s
    #   timeout: 20s
    #   retries: 3
  sparse-embeddings:
    container_name : sparse-embeddings
    # depending on GPU architecture you may need to change the prefix for the version tag, on older
    # GPUs you may need to use `turing-<VERSION>
    # FIXME: downgrading the image to 1.4 seems to fix the issue with https://github.com/huggingface/text-embeddings-inference/issues/347
    image: ghcr.io/huggingface/text-embeddings-inference:turing-1.4-grpc
    command: --model-id  naver/efficient-splade-VI-BT-large-query --json-output --pooling splade
    volumes:
      - embedding_data:/data 
    ports:
      - "8081:80" 
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    pull_policy: always
    # TODO : grpc healthcheck
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
    #   interval: 30s
    #   timeout: 20s
    #   retries: 3
  reranker:
    container_name : reranker-base
    # depending on GPU architecture you may need to change the prefix for the version tag, on older
    # GPUs you may need to use `turing-<VERSION>
    image: ghcr.io/huggingface/text-embeddings-inference:turing-1.4-grpc
    # payload size is (CHUNK_SIZE + CHUNK_OVERLAP) * K * 1000, where k is the maximum similarity search limit 
    # in this case (800+80) * 60 * 1000 = 52800000
    command: --model-id  BAAI/bge-reranker-base --payload-limit 528000000000 --json-output
    volumes:
      - embedding_data:/data 
    ports:
      - "8082:80" 
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    pull_policy: always
    # TODO : grpc healthcheck
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:8082/health"]
    #   interval: 30s
    #   timeout: 20s
    #   retries: 3
volumes:
  embedding_data: